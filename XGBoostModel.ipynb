{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "TUVgV0r9UUZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx6W9ZwPUI7M"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_auc_score,\n",
        "    f1_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "nba_data = pd.read_csv(\"nba_rest_data.csv\")\n",
        "nba_data['rest_category'] = pd.factorize(nba_data['rest_category'])[0]\n",
        "\n",
        "# Seeds to loop through (randomly generated)\n",
        "seeds = [2287, 1680, 8936, 1425, 9675]\n",
        "\n",
        "# Store metrics for averaging later\n",
        "all_accuracies = []\n",
        "all_f1_scores = []\n",
        "all_roc_aucs = []\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"\\nRunning experiment with seed = {seed}\")\n",
        "\n",
        "    # Split data\n",
        "    unique_teams = nba_data['Team'].unique()\n",
        "    train_teams, test_teams = train_test_split(unique_teams, test_size=0.2, random_state=seed)\n",
        "\n",
        "    train_data = nba_data[nba_data['Team'].isin(train_teams)]\n",
        "    test_data = nba_data[nba_data['Team'].isin(test_teams)]\n",
        "\n",
        "    X_train = train_data[['rest_category']].values\n",
        "    y_train = train_data['WIN'].values\n",
        "    X_test = test_data[['rest_category']].values\n",
        "    y_test = test_data['WIN'].values\n",
        "\n",
        "    # Class imbalance ratio\n",
        "    neg, pos = np.bincount(y_train)\n",
        "    scale_pos_weight = neg / pos\n",
        "\n",
        "\n",
        "    def objective(trial):\n",
        "        param = {\n",
        "            'objective': 'binary:logistic',\n",
        "            'eval_metric': 'logloss',\n",
        "            'max_depth': trial.suggest_int('max_depth', 2, 15),\n",
        "            'eta': trial.suggest_float('eta', 0.005, 0.3),\n",
        "            'gamma': trial.suggest_float('gamma', 0, 10.0),\n",
        "            'subsample': trial.suggest_float('subsample', 0.3, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
        "            'lambda': trial.suggest_float('lambda', 1e-3, 10),\n",
        "            'alpha': trial.suggest_float('alpha', 1e-3, 10),\n",
        "            'scale_pos_weight': scale_pos_weight,\n",
        "            'seed': seed\n",
        "        }\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "        scores = []\n",
        "\n",
        "        for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "            X_t, X_val = X_train[train_idx], X_train[val_idx]\n",
        "            y_t, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "            dtrain = xgb.DMatrix(X_t, label=y_t)\n",
        "            dval = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "            num_boost = trial.suggest_int(\"num_boost_round\", 100, 1000)\n",
        "\n",
        "            bst = xgb.train(\n",
        "                param,\n",
        "                dtrain,\n",
        "                num_boost_round=num_boost,\n",
        "                evals=[(dval, \"eval\")],\n",
        "                early_stopping_rounds=30,\n",
        "                verbose_eval=False\n",
        "            )\n",
        "            preds = bst.predict(dval)\n",
        "            auc = roc_auc_score(y_val, preds)\n",
        "            scores.append(auc)\n",
        "\n",
        "        avg_score = np.mean(scores)\n",
        "        print(f\"Trial {trial.number}: ROC AUC={avg_score:.4f}, Params={param}\")\n",
        "        return avg_score\n",
        "\n",
        "    # Optuna hyperparameter tuning\n",
        "    study = optuna.create_study(\n",
        "        direction='maximize',\n",
        "        sampler=optuna.samplers.TPESampler(seed=seed)\n",
        "    )\n",
        "    study.optimize(objective, n_trials=200)\n",
        "\n",
        "    print(\"\\nBest hyperparameters:\")\n",
        "    print(study.best_params)\n",
        "\n",
        "    best_params = study.best_params.copy()\n",
        "    num_boost_round_final = best_params.pop(\"num_boost_round\")\n",
        "\n",
        "    best_params.update({\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'logloss',\n",
        "        'use_label_encoder': False,\n",
        "        'seed': seed\n",
        "    })\n",
        "\n",
        "    dtrain_final = xgb.DMatrix(X_train, label=y_train)\n",
        "    dtest_final = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "    xgb_model = xgb.train(\n",
        "        params=best_params,\n",
        "        dtrain=dtrain_final,\n",
        "        num_boost_round=num_boost_round_final\n",
        "    )\n",
        "\n",
        "    preds_final = xgb_model.predict(dtest_final)\n",
        "    preds_binary_final = (preds_final > 0.5).astype(int)\n",
        "\n",
        "    accuracy_final = accuracy_score(y_test, preds_binary_final)\n",
        "    roc_auc_final = roc_auc_score(y_test, preds_final)\n",
        "    f1_final = f1_score(y_test, preds_binary_final)\n",
        "\n",
        "    print(f\"\\nFinal Model Accuracy: {accuracy_final * 100:.2f}%\")\n",
        "    print(f\"Final ROC AUC Score: {roc_auc_final:.4f}\")\n",
        "    print(f\"Final F1 Score: {f1_final:.4f}\")\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, preds_binary_final))\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, preds_binary_final))\n",
        "\n",
        "    xgb.plot_importance(xgb_model)\n",
        "    plt.title(f\"Feature Importance (Seed {seed})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Save metrics for averaging\n",
        "    all_accuracies.append(accuracy_final)\n",
        "    all_f1_scores.append(f1_final)\n",
        "    all_roc_aucs.append(roc_auc_final)\n",
        "\n",
        "# Print average metrics across all seeds\n",
        "print(\"\\n=== Average Metrics Across All Seeds ===\")\n",
        "print(f\"Mean Accuracy: {np.mean(all_accuracies) * 100:.2f}%\")\n",
        "print(f\"Mean F1 Score: {np.mean(all_f1_scores):.4f}\")\n",
        "print(f\"Mean ROC AUC Score: {np.mean(all_roc_aucs):.4f}\")"
      ]
    }
  ]
}